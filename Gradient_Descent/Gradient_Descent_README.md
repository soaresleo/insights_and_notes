<h1>RE-commented Notebook for the "Implemeting the Gradient Descent Algorithm" from Udacity</h1>

This notebook and .csv file are part of the Lesson 2 of <b><a href="https://www.udacity.com/course/deep-learning-pytorch--ud188">Intro to Deep Learning with Pytorch</a></b> course from Udacity. 

In one of its quizzes, students have to implement some functions that later on code will be used as parts of a training funcion for a perceptron with sigmoid as activation formula and gradient descent as optimization process. This file contains the solutions for these implementations, but this isn't its main objective, as I explain below.

Coding these functions was an easy task, but understanding underlying roles of the rest of the code written by Udacity was pretty tricky, especially for someone with medium-to-advanced skills in python and low knowledge level on Deep Learning as I am by the time I write it.

The original author of the code had already commented some lines, but the information provided wasn't sufficient for really understand most of terms and funcions (just to clarify, this is not bad once the course was not built for teaching Python, and it forces students to review and train their coding skills). So I spent some time studying the code line by line and once I fully understand it I found useful to (re)comment and share in order to helping anyone that is having the same difficults.

The original .ipynb file can be found in <a href="https://github.com/udacity/deep-learning-v2-pytorch/tree/master/intro-neural-networks/gradient-descent">this link</a>.

The resourses needed are:

- Python 3+
- <a href="https://numpy.org/doc/stable/">Numpy 1.19</a>
- <a href="https://pandas.pydata.org/docs/">Pandas 1.0.5</a>
- .csv file that can be found in this repo.
